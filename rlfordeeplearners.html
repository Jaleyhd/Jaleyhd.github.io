<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Randomness</title>
	<link rel="stylesheet" href="styles/monokai-sublime.css">
	<script src="highlight.pack.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>
    <!-- Bootstrap Core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Theme CSS -->
    <link href="css/clean-blog.min.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
    <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
    <script src="js/jquery-min.js"></script>
    <script src="js/d3-min.js"></script>
    <script src="https://cdn.plot.ly/plotly-1.2.0.min.js"></script>


    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
    </script>

    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    Menu <i class="fa fa-bars"></i>
                </button>
                <a class="navbar-brand" href="index.html">Jaley Dholakiya</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="index.html">Home</a>
                    </li>
                    <li>
                        <a href="about.html">About</a>
                    </li>
                    <li>
                        <a href="contact.html">Contact</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header class="intro-header" style="background-image: url('img/post-bg.jpg')">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <div class="post-heading">
                        <h1>Modelling RL Task as Deep Learning Problem</h1>
                        <h2 class="subheading">Introduction to Deep RL for Deep Learners</h2>
                        <span class="meta">Posted by <a href="#">Jaley Dholakiya</a> on Aug 18, 2017</span>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- Post Content -->
    <article>
        <div class="contentainer">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <h2 class="subheading">Spoiler Alerts for GOT Fans</h2>
                    It is highly recommended to watch Game of thrones season 1 to 7 before going ahead. This blog is for GOT Fans who want to build AI Whitewalkers for army of dead.
                    <h2 class="subheading">Pandora Box of Reinforcement Learning</h2>
                        <p>Reinforcement learning ! Sounds sassy, doesn't it ? Well it actually is. Imagine if you could teach a machine how to behave in an environment based on rewards or punishments. Just like kiddo learning to backoff from bonfire after burning his hands once. But how does it actually work. Lets grab some popcorns and heed to the dead voice.</p>
                <h2 class="subheading"> RL in NutShell </h2>
                <p>Let us say Daenerys Targaryen is the <b> agent </b> we are interested in. We all know what happens when she says dracarys. The fury is unleashed and the <b>environment</b> changes in GOT. Each change in situation has a <b>reward/penalty</b> associated for Daenerys or <b>agent</b>. This makes improve her <b>policies</b>(of choosing action) and  take better actions next time along with tyrion. And after the fury, she moves to a new kingdom or <b>next state</b>. </p>
                <h2 class="subheading"> Must know terms in RL </h2>
                <b>Policy : </b> To be demystified after Markov Reward Process<br/>
                <b>State Value Function $(Vs_{\pi})$</b> For a given policy and environment, How good is it to be in a state? It can answer question John Snow is lingering with . .  . Which state/place is more rewarding, Winterfell or Dragonstone? <br/>
                <b>Action Value Function $(Va_{\pi}(s))$</b> For a given policy and environment, How good is it to take action <b>a</b> from state <b>s</b>?
                <h2 class="subheading"> Rewards and Penalties </h2>
                Before we go to some deep stuff, Let us understand, what is a reward. Reward is bounty associated with reaching a state. Its like the money recieved by cowboys on getting the all goats from medows to downhill. It is also the dopamine spike you get on getting your Facebook post published. There is an immediate reward <b>R</b> ($R(S_t)$), and there is a discounted Reward <b>r</b> $(r(S_t) = R(S_t)+\gamma R(S_{t+1}) + \gamma^2 R(S_{t+2}) \cdots )$. Discounted Reward is like what you get when you post controversial posts on Facebook, Immediate Rewards are good, but future rewards are extremely negetive which eventually makes you rethink in the first place about posting it. Discount is the amount of future you want to consider in your calculation. $\gamma = 1 $ means that you are considering everything in future, and $\gamma = 0$ means you are not thinking about future at all, you are only concerned with immediate reward. In practical scenarios, we keep it at around $0.9$. 

                    <img src="img/starksvslannisters.jpg" class="img-responsive center-block" style='height: 100%; width: 60%; object-fit: contain' />
                    <h2 class="subheading">Markov Chain and the king's Landing</h2>
                    <p>
                    	We all know the story of mad king, right? Ofcourse you need to watch GOT for that. Kings landing is highly fought out place. Lets assume that there are only two kingdoms contesting - Lannisters and Starks. Now assume that starks are holding it right now. What is the probability of them holding it in the future? Volla, Markov chain will tell it to us. 
                    </p>
                    	Look into the code bellow : It is pretty much self expalatory. 
					                        
                <pre class="python">
                <code>
import numpy as np
# S: State Value of house : Its probability of retaining kings landing
# P: Transition Matrix : from houseA to houseB, what is probability of transistion.
houses = ["lannisters","starks"]
S = {"lannisters": 0, "starks":1}
P = {"from_lannisters":{"to_lannisters": 0.8, "to_starks" : 0.2 }, 
     "from_starks": {"to_starks": 0.1, "to_lannisters": 0.9}}
for i in range(5):
    lannisters_future_stateval, starks_future_stateval = 0,0;
    
    lannisters_future_stateval = S["lannisters"]*P["from_lannisters"]["to_lannisters"]+\
                                S["starks"]*P["from_starks"]["to_lannisters"];
    
    starks_future_stateval =  S["lannisters"]*P["from_lannisters"]["to_starks"]+\
                                S["starks"]*P["from_starks"]["to_starks"];
    S["lannisters"]=lannisters_future_stateval
    S["starks"] = starks_future_stateval

print "Volla, '%s' have been winners with %0.2f%% probability to be in king's landing "%(max(S, key=S.get),100*max(S.values()))
				</code>
                </pre>
                <p>In the above code, <b>S</b> is initiallized with [0,1] which means sharks are holding it. <b>P</b> stands for transition probability matrix.  Therefore, <i><b>P</b>["from_lannisters"]["to_starks"]</i>  denotes the probability of succession of kings landing by starks. . Let us look at series of events in the history of succession of king's Landing.</p>
                <p>
                	<i>Lannisters -> Starks -> Lannisters -> Lannisters -> Lannisters -> Starks -> Lannisters -> Lannisters -> . . . </i>
                </p>
                <p>This chain of successions is modelled and explained by the the transition matrix. What we are interested in is, steady state value of Starks or Lannisters to be in kings landing. So we start of with starks as rulers, but putting the historic truths in place to decide the fate of king's landing. We discover that Lannisters are 81% probable of being in kings landing at any point of time. You can also use this model your food habbits.</p>


                <h2 class="subheading"> Bellman Optimality Equation </h2>
                Bellman optimality equation for Markov Process states, that let us say we have 7 kingdoms fighting wars. The best measure to know which kingdom has an upperhand, is by <br/>
                <b> Step 1 </b> giving each kingdom a number (Initial State Value ) <br/>
                <b> Step 2 </b> Updating the value based on probability ( by past records) of my attacks on my neighborhood Kingdoms <br />
                $V_{\pi}$(lannisters) = p(attacking winterfell)*[immidiate reward of attacking winterfell + $\gamma$*State Value  of winterfell]+same for other neighboring kingdoms  . . . <br/> 
                <b> Step 3 </b> Keep repeating until convergence.

                <p>It is perfectly fine, if the optimality equation doesn't make any detailed sense right now. It is deliberately left open ended, because the equation keeps changing as per the needs and model. All you need to remember is, that there is some graphical evaluation that is happening over the states in order to obtain state value . This value mysteriously captures and assimilates all information required to make judgement about rewards, etc. This comes directly from markovian property which rips apart previous states to next states, past from future, if the present state value is provided. </p>
                <h2 class="subheading">Markov Reward Process for John Snow's Journey</h2>
                <p>Remember the first time when Fire and Ice Met? I am talking about John snow meeting Daenerys Targaryen ! Well we are talking about his return from Dragonstone to beyond the walls. The part very brief in series itself.  So the states here are <i>[Dragonstone,WhiteHarbor, Winterfell]</i>. You can either go to winterfell via walk or you can go via sea to whiteharbor and then go to winterfell. But there is always some danger of spies lurking around at whiteharbor from the spies and pirates.</p>
                <p>
                	Markov reward process is nothing new, rather its same old markov chain, bundled with immediate rewards associated with reaching states(in our case, its reaching winterfell, reaching whiteharbor,etc). What we calculate is, that <b><i>how rewarding is it for John Snow to be at a particular state/place?></i></b>. In other words we will calculate, State Value, which represents how rewarding it is to be at that State.
                </p>
                <h3>Calculations for the above  MRP </h3>
                $G_s = R_{s(t+1)} + \gamma R_{s(t+2)} + \gamma^2  R_{s(t+3)}\cdots $<br/>
	$v(s)=R_s+\gamma\sum_{s'\in S}P_{ss'}v(s')$ <br/>
	Above equations tell you how to update state value in a given environment.<br/>
	$\begin{bmatrix}
	v(1)\\
	\vdots\\
	v(n)
	\end{bmatrix}=\begin{bmatrix}
	R(1)\\
	\vdots\\
	R(n)
	\end{bmatrix}+\gamma
	\begin{bmatrix}
	p_{11} & \cdots & p_{1n}\\
	\vdots & \ddots & \vdots\\
	p_{n1} & \cdots & p_{nn}
	\end{bmatrix}
	\begin{bmatrix}
	v(1)\\
	\vdots\\
	v(n)
	\end{bmatrix} $ <br/>
	<p>For Linear Algera fans, the above equation may come as a blessing. It states how you can concurrently update all state value's vector by doing right product of probabilitistic Transition Matrix with itself. When it reaches steady State, the state values vector becomes a scaled eighen Vector of this transition matrix. Isn't that the very definition of eighen vector? $Av = \lambda v$</p>

                <img src="img/markovianGOT.jpg" class="img-responsive center-block" style='height: 100%; width: 100%; object-fit: contain' />
                <p>In the code, bellow, we have two extra states, called terminating states. Which means, its the end of the journey,dead or alive. <b>R</b> tells you the reward of being in a particular state. </p>
                <pre class="python">
                <code>
# Markov Reward Process Example
# Finding Rewarding states for journey of John Snow from dragonstone to winterfell
import numpy as np
import copy

S ={"dragonstone": 1,"whiteharbor":1, "winterfell":1} # Active States
T=["alive-terminal","dead-terminal"] # Terminal States
R = {"dragonstone": 0,"whiteharbor":10, "winterfell":50,"alive-terminal":100,"dead-terminal":-50}
P ={"from_dragonstone":{"to_whiteharbor":0.5,"to_winterfell":0.1,"to_dead-terminal":0.4},\
    "from_whiteharbor":{"to_winterfell":0.1,"to_dead-terminal":0.9},
   "from_winterfell":{"to_alive-terminal":0.9,"to_dead-terminal":0.1}} # Probabilistic Transition Matrix
gamma = 0.1

for i in range(45):
    S1 = copy.copy(S);
    for castle in S1.keys():
        reward =0;
        from_castle='from_'+castle
        future_states = P[from_castle]
        for to_castle in future_states:
            reward=reward+P[from_castle][to_castle]*R[to_castle[3:]];
        
        S1[castle] = S[castle]+ gamma*(reward-S1[castle]) 
    S = copy.copy(S1);
    print S

                </code>
                </pre>


    			<p>Note that in above Markov Reward Process, we are <b>not deciding</b> our actions at any point. We are only <b>evaluating</b> the state value given an environment(i.e Transition Matrix, Immediate Rewards). The policy comes into picture only in context of Markov Decision Process, not Markov Reward Process. The power to decide comes, when we have freedom to act. Lets look into Markov Decision Process now. </p>
                <h2 class="subheading">Markov Decision Process - Time to act! </h2>
MDP breaks the shakles of static probabilistic policy, by learning actions. Every state can take multiple action, and each action is followed by the next state to be chosen is probabilistically. In GOT terms, we can choose to fly directly to winterfell, or we can choose to go via sea, or we can go via land. Now action is left to <b>policy</b> for deciding. Each action branches out to multiple states. You may ask "How can an action lead to further states(multiple)? ". You may choose to boat from dragonstone, but you can land in death (if pirates are there) or you may landup in whiteharbor. Each action can lead you to further states.
                <h3 class="subheading">Markov Decision Process - Time to act! </h3>
				The mathematics of MDP and MRP are similar, but it is the context which sets it apart. MDP gives you choice. We learn policy of taking a particular action in MDP. <br/>
				MDP can be learned by these two algorithm - <br/> 
				<b>Value Iteration</b> <br/>
				<b>Policy Iteration</b> <br/>
				In value iteration, you don't learn policy seperately. You considering most rewarding action as the policy as well as use it for updation of state values. It tends to be a bit slower, because we are changing the best action again and again leading to turbulance. It eventually does converge, but the time taken will be generally more than Policy Iteration. Unlike Value Iteration, Policy Iteration chooses a policy, Evaluates for it, until convergence. Now it chooses a new policy based on these steady state values, and then evaluate the new policy. We keep doing this back and forth until convergence. As you can observe, there are two steps here : policy evaluation and policy update. 
				<h3 class="subheading">John Snow's way home Revisited </h3>
				John Snow now wants to carry MDP machine to tell him, which action is best in a given place. The actions he can take in general are going by land, going by sea, or flying on dragon. For each action, there is a possibility to land up at different places/state. What we will learn is the best policy to be choosen from any state/place. The code is self explainatory, so you can just run this code on jupyter and know it for yourself. I am assuming all of you have numpy installed, which sounds quite reasonable.	
				<h3 class="subheading">Solution by Value Iteration</h3>
				<pre class="python">
				<code>
# Markov Decision Process Example
import numpy as np
import copy

S ={"dragonstone": 1,"whiteharbor":1, "winterfell":1} # Active States
T=["alive-terminal","dead-terminal"] # Terminal States
R = {"dragonstone": 0,"whiteharbor":10, "winterfell":50,"alive-terminal":100,"dead-terminal":-50}
P ={"from_dragonstone":{"land":{"to_winterfell":0.5,"to_dead-terminal":0.5},\
                         "sea":{"to_whiteharbor":0.1,"to_dead-terminal":0.9},\
                         "dragon":{"to_winterfell":0.95,"to_dead-terminal":0.05}},\
    "from_whiteharbor":{"land":{"to_winterfell":0.6,"to_dead-terminal":0.4}},
   "from_winterfell":{"land":{"to_alive-terminal":0.9,"to_dead-terminal":0.1}}}
gamma = 0.1
A ={"from_dragonstone":{"land":1,\
                         "sea":1,\
                         "dragon":1},
    "from_whiteharbor":{"land":1},
   "from_winterfell":{"land":1}}

Policy = {"from_dragonstone":"land","from_whiteharbor":"land","from_winterfell":"land"}


# Solution by Value Iteration
for i in range(15):
    S1 = copy.copy(S);
    
    for castle in S1.keys():
        from_castle='from_'+castle
        future_states = P[from_castle]
        action_reward={}
        for action in A[from_castle]:
            reward =0;
            for to_castle in future_states[action]:
                reward=reward+P[from_castle][action][to_castle]*R[to_castle[3:]];
            action_reward[action]=reward
        Policy[from_castle]=max(action_reward,key=action_reward.get)
        S1[castle] = S[castle]+ gamma*(max(action_reward.values())-S1[castle]) 
    S = copy.copy(S1);
    print 'Value Function',S
    print 'Learned Policy',Policy

				</code>
				</pre>

				<h3 class="subheading">Solution by Policy Iteration</h3>

				<pre class="python">
				<code>

				#Solution by Policy Iteration    
Policy = {"from_dragonstone":"land","from_whiteharbor":"land","from_winterfell":"land"}
for i in range(50):
    S1 = copy.copy(S);
    
    #Step 1 : Update Value Fuction
    for castle in S1.keys():
        from_castle='from_'+castle
        future_states = P[from_castle]
        action = Policy[from_castle] #Based on policy
        for to_castle in future_states[action]:
            reward=reward+P[from_castle][action][to_castle]*R[to_castle[3:]];
        action_reward[action]=reward
        S1[castle] = S[castle]+ gamma*(reward-S1[castle]) 
    S = copy.copy(S1);
    #Step 2 : Update Policy
    for castle in S1.keys():
        from_castle='from_'+castle
        future_states = P[from_castle]
        action_reward={}
        for action in A[from_castle]:
            reward =0;
            for to_castle in future_states[action]:
                reward=reward+P[from_castle][action][to_castle]*R[to_castle[3:]];
            action_reward[action]=reward
        Policy[from_castle]=max(action_reward,key=action_reward.get) #Update Policy

    
    print 'Value Function',S
    print 'Learned Policy',Policy

				</code>
				</pre>


				So let me know your feedback. There will be two more posts on the same topic in these couple of months. Have fun , and keep your fingers crossed, until we have last episode of 7th season of GOT published :) Hope that I can share how I dreamed ML through my blog, and also read from other dreamers alongside. :)
                </div>
                <script>
                    TESTER = document.getElementById('tester');
                    var layout = {
                      title: 'Line Dash',
                      yaxis: {
                        range: [0, 10],
                      },
                    };
                    Plotly.plot( TESTER, [{
                    x: ['0', '1', '2', '3'],
                    y: [1/8, 3/8, 3/8, 1/8] }], {
                    margin: { t: 0 }
                },layout );
                </script>
            </div>
        </div>
    </article>

    <hr>

    <!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <ul class="list-inline text-center">
                        <li>
                            <a href="#">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li>
                            <a href="#">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li>
                            <a href="#">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    </ul>
                    <!--p class="copyright text-muted">Copyright &copy; Your Website 2016</p-->
                </div>
            </div>
        </div>
    </footer>
<div id="disqus_thread"></div>
<script>

var disqus_config = function () {
this.page.url = "http://jaleyhd.github.io/rlfordeeplearners.html";  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = "19August2017RLForDeepLearners"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://jaleyhd.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
 

    <!-- jQuery -->
    <script src="vendor/jquery/jquery.min.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="vendor/bootstrap/js/bootstrap.min.js"></script>

    <!-- Contact Form JavaScript -->
    <script src="js/jqBootstrapValidation.js"></script>
    <script src="js/contact_me.js"></script>

    <!-- Theme JavaScript -->
    <script src="js/clean-blog.min.js"></script>
    <script src="js/vis/blog1.js"></script>
<script id="dsq-count-scr" src="//jaleyhd.disqus.com/count.js" async></script>

</body>

</html>

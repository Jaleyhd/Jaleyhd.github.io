<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Understanding Policy</title>
  <link href="/assets/bootstrap.min.css" rel="stylesheet">
  <link href="/assets/main.css" rel="stylesheet">

  <meta name="description" content="Learning Policy">

  <link rel="stylesheet" href="/assets/clean-blog.min.css" rel="stylesheet">
  <link rel="canonical" href="http://localhost:4000/deeprltutorial/part2/">
  <link rel="alternate" type="application/rss+xml" title="CrazyMuse" href="/feed.xml">
  <link href="/assets/font-awesome.min.css" rel="stylesheet" type="text/css">
  <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">

  <script src="/assets/jquery-min.js"></script>
    <script src="/assets/d3-min.js"></script>

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
  </script>
  <script src="/assets/bootstrap.min.js"></script>

  <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script src="/assets/clean-blog.min.js"></script>
  <script src="/assets/blog1.js"></script>

  
</head>
<body>
      <!-- Navigation -->
    <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    Menu <i class="fa fa-bars"></i>
                </button>
                <a class="navbar-brand" href="https://www.github.com/jaleyhd">Crazy Muse</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href=http://localhost:4000/index.html >Home</a>
                    </li>
                    <li>
                        <a href=http://localhost:4000/about/index.html >About</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>
</body>

  <body>

        <header class="intro-header" style="background-image: url('/assets/img/post-bg.jpg')">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <div class="post-heading">
                        
                        <h1>Understanding Policy</h1>
                        
                        
                        <h3><i>Deep RL Tutorial part-2</i></h3>
                        

                    </div>                    
                </div>
            </div>
        </div>
    </header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <p class="post-meta">
      <time datetime="2018-01-10T20:15:00+05:30" itemprop="datePublished">
        
        Jan 10, 2018
      </time>
      </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <h2 id="learning-policy">Learning Policy</h2>

<p><img src="/assets/img/ratinmap.png" alt="Rat in Map" /></p>

<script type="math/tex; mode=display">\pi (s) : S \rightarrow A</script>

<p>Our dear rat, is on a mission to discover most rewarding path to cheese. He can either climb up through the curtains and jump on kitchen floor, or he can climb via plants in balcony and enter kitchen from window. To learn the most optimal policy is rat’s goal, and is also the goal of Reinforcement Learning. A policy charts out the action to be taken in a given state. Think of it like SIRI in navigation. At each turn(state), SIRI tells you the next best action.</p>

<p>In the next section,  we will learn bellman equations for  Value Iteration as a way of getting best policy.</p>

<h2 id="bellman-equations">Bellman Equations</h2>
<p><em>Bellman Optimality Equation for Value Iteration</em></p>

<p>If you can see, the above thug life image of our rat, he is currently in state S1, and we need to find a way to update its state values.</p>

<script type="math/tex; mode=display">V^{(i+1)}(s_1) \leftarrow  \underbrace{max}_{s \in \{s_2,s_3\}} R_{a(s_1,s)} + \gamma  V^{(i)}(s)</script>

<script type="math/tex; mode=display">\pi^{(i+1)}(s_1) = \text{action}\in \{ a_1,a_2 \} \text{ with max future value } \{ V(s_1),V(s_2) \}</script>

<p>It means we change our policy greedily based on higher Value. Let us say $R_2+\gamma V(s_2)$ is 40, and $R_3+\gamma V(s_3)$ is 50, then we select action $a_3$ as it leads us to higher value. In short you update Values of states first and then update policy. You keep repeating these steps simultaneously(unlike policy iteration), until the value and policy converges.</p>

<p>I know, math has started picking up pace, but there is one additional concept which needs to be brought. The outcome of action may be probabilistic. This means, our dear rat may jump from curtain to kitchen, but it can have a crash too. So, these cruel mathematicians added another layer of complexity, with outcome of action being probabilistic. This means same acton a, can lead you to different states probabilistically. For example there a long pit in front of you, the action is jumping, but there is 50-50 chances of either going across the pit, or landing in the pit with mud water.</p>

<p><img src="/assets/img/jumpingratmdpcropped.jpg" alt="Rat jumping from curtain" /></p>

<table>
  <thead>
    <tr>
      <th>Bellman Expectation Equation</th>
      <th>Bellman Optimality Equation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Evaluating a given policy</td>
      <td>Involves learning an optimal policy</td>
    </tr>
    <tr>
      <td><script type="math/tex">\scriptsize v_{\pi}^{(i+1)}(s) \leftarrow E_{\pi}\{R_{next}+\gamma v_{\pi}(s_{next})\}</script></td>
      <td><script type="math/tex">\scriptsize v^{(i+1)}(s) \leftarrow max\{ R_{next}+\gamma v(s_{next}) \}</script></td>
    </tr>
    <tr>
      <td><script type="math/tex">\scriptsize v_{\pi}^{i+1}(s) \leftarrow E_{\pi}\{R_{sa}+\gamma \sum\limits_{s'\in S} p_{sas'}v_{\pi}(s')\}</script></td>
      <td><script type="math/tex">\scriptsize v^{i+1}(s) \leftarrow max\{ R_{sa}+\gamma \sum\limits_{s'\in S} p_{sas'}v_{\pi}(s') \}</script></td>
    </tr>
    <tr>
      <td>where $a = \pi(s)$</td>
      <td>where $a$ is action that offshoots from s.</td>
    </tr>
    <tr>
      <td>No policy is updated</td>
      <td>Update $\pi(s)\leftarrow a$ with action ($a$) corresponding to maximum value in above stated value update.</td>
    </tr>
  </tbody>
</table>

<p>Bellman equations give us a way of updating the value. They are not a set of equations which are restricted to only RL. They are typically associated with updates in DP (Dynamic Programming) problems. In value iteration, we simply select the policy  based on the action which higher expected reward from a given state. And we keep overwriting this expected reward to current value associated with state. This keeps going until we have reached equilibrium or a place where value is same as highest expected reward, and meanwhile the policy corresponding to this steady state is also optimal. <em>You can look up David Silver’s notes to get proof of why convergence of value in Value Iteration also implies convergence of policy.</em></p>

<blockquote>
  <p><strong>ALGORITHM-1 | VALUE ITERATION</strong></p>

  <p>Initialize V(s) $\forall s \in S$<br />
i = 0;<br />
<strong>while</strong> values converge:<br />
     <script type="math/tex">v^{(i+1)}(s) \leftarrow max \{ R_{sa}+\gamma \sum\limits_{s'\in S} p_{sas'}v_{\pi}(s') \}</script>          <em>(Value Update)</em><br />
     <script type="math/tex">\pi (s) \leftarrow \underset{a}{argmax} \{ R_{sa}+\gamma \sum\limits_{s'\in S} p_{sas'}v_{\pi}(s') \}</script>          <em>(Policy Update)</em><br />
<strong>end while</strong></p>

  <p><em>Note that here $R_{next}$ is reward for taking actions from current state. $s_{next}$ is one of the possible states an action can land it into</em></p>

</blockquote>

<p>In the above equation, you can clearly observe that in each iteration policy and value update are happening. But there is a better way of doing it. What if we first fix the policy and let the values converge (lets say x number of iterations). Then used these converged values to update policy. Doesn’t it seem better than constantly changing both policy and value together?</p>

<h3 id="policy-iteration">Policy Iteration</h3>
<p>Let’s say you are in exam hall for derivation. There are 5 different ways(policy) of completing exam paper. Some people complete 1 markers first, some 5 markers first, etc. If you constantly keep changing policy in head, it is difficult to reach convergence/complete paper properly. Something similar happens in Policy Iteration.</p>

<p><img src="/assets/img/policyiteration.png" alt=" Policy Iteration " /></p>

<p>As shown in above illustration, policy iteration has two phases. Value Update and Policy Update. Previously in Value Iteration, we updated policy and value simultaneously. Policy Iteration is generally a better choice than Value Iteration. The pseudo-code of policy iteration is as given bellow.</p>

<p>The question comes up, can we learn action values instead of state values? The answer is yes. But it will converge with order of complexity <script type="math/tex">O(\#a^2.\#s^2)</script>, which is slower as compared to 
<script type="math/tex">O(\#a.\#s^2)</script> in state value based Update.<br />
<em>Note that there are lot of advantages of action values, which will uncover when we discuss Model Free Methods</em></p>

<blockquote>
  <p><strong>ALGORITHM-2 | POLICY ITERATION</strong></p>

  <p>Initialize V(s) $\forall s \in S$<br />
i = 0;<br />
<strong>while</strong> policy converges:          <em>(policy same as before $\forall$ states)</em><br />
     <strong>while</strong> values converge:<br />
         <script type="math/tex">v^{(i+1)}(s) \leftarrow max \{ R_{sa}+\gamma \sum\limits_{s'\in S} p_{sas'}v_{\pi}(s') \}</script>          <em>(Value Update)</em><br />
    <strong>end while</strong><br />
     <script type="math/tex">\pi (s) \leftarrow \underset{a}{argmax} \{ R_{sa}+\gamma \sum\limits_{s'\in S} p_{sas'}v_{\pi}(s') \}</script>          <em>(Policy Update)</em><br />
<strong>end while</strong></p>
</blockquote>

<p>Action values, also called q values are associated with state-action pair and not the action alone. The action is always with respect to some state. If you see the bellow unrolled illustration of value update, you can observe that maximization is happening at secondary nodes in case of action value update.</p>

<table>
  <thead>
    <tr>
      <th>State Value Update</th>
      <th>Action Value Update</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="/assets/img/bellmanequationstatevaluefn.png" alt="State Value Update unrolled " /></td>
      <td><img src="/assets/img/bellmanequationactionvaluefn.png" alt="Action Value Update unrolled" /></td>
    </tr>
    <tr>
      <td><script type="math/tex">\scriptsize v^{i+1}(s) \leftarrow max \{R_{sa}+\gamma \sum\limits_{s'\in S} p_{sas'}v_{\pi}(s') \}</script></td>
      <td><script type="math/tex">\scriptsize q^{i+1}(s,a) \leftarrow R_{sa}+\gamma \sum\limits_{s'\in S} p_{sas'}\underset{a'}{max} \{q_{\pi}(s',a') \}</script></td>
    </tr>
    <tr>
      <td>Update policy based on action with max expected reward or <script type="math/tex">\scriptsize \underset{a}{argmax} \{ R_{sa}+\gamma \sum\limits_{s'\in S} p_{sas'}v_{\pi}(s') \}</script></td>
      <td>Update policy based on  action with higher $q(s,a)$ value</td>
    </tr>
  </tbody>
</table>

<p>For instance, <script type="math/tex">\small v_{s_1} = max\{R_{s,a_1}+ 0.5v_2+0.5v_3 ,R_{s,a_2}+ 0.5v_4+0.5v_5 \}</script> for state value update  and   <script type="math/tex">\small q_1 = R_{s,a_1} +  0.5*max\{ q_2,q_3 \} + 0.5*max\{ q_4,q_5 \}</script> for value update illustration shown above.</p>

<h3 id="limitations-of-mdp">Limitations of MDP</h3>
<p>In many real time scenario, we don’t know these transition probabilities. Especially in dynamic scenarios like traffic behavior, action’s outcome is difficult to be modeled. So the question comes up, if we can do something better than traditional MDP, by dealing with model free approaches. What this means is that we refuse to assume anything about outcome of an action from a given state. The obvious question is that how do we learn in such scenarios? Our dear rat is getting bored by watching so many equations, so he decided to stop all this mathematics, and just goes to collect cheese the way he knowns. Of course, he has bruises and burns, but he keeps unrolling his episodes of cheese finding skills. We are going to use the same approach by dropping all this assumption business and directly learn from episodes of exploration(greedy or otherwise).</p>

<blockquote>
  <p>Curtain(S) $\rightarrow$ jump(A) $\rightarrow$ knee-injury(R)  $\rightarrow$ ground(S) $\rightarrow$ run(A) $\rightarrow$ $\cdots$</p>
</blockquote>


  </div>

  
</article>

 
  <div id="disqus_thread"></div>
  <script>

  var disqus_config = function () {
  this.page.url = "http://crazymuse.github.io/deeprltutorial/part2/";  // Replace PAGE_URL with your page's canonical URL variable
  this.page.identifier = "/deeprltutorial/part2/"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };

  (function() { // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');
  s.src = 'https://jaleyhd.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">CrazyMuse</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              CrazyMuse
            
            </li>
            
            <li><a href="mailto:jaley.dholakiya@gmail.com">jaley.dholakiya@gmail.com</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/crazymuse"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">crazymuse</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/jaleyhd"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">jaleyhd</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Isha Meditator, DeepLearning@Samsung-Harman, Knowledge Farmer, Veggie Chef</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>

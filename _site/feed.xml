<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-01-09T02:46:25+05:30</updated><id>http://localhost:4000/</id><title type="html">CrazyMuse</title><subtitle>Isha Meditator, DeepLearning@Samsung-Harman, Knowledge Farmer, Veggie Chef</subtitle><entry><title type="html">Topic Modelling</title><link href="http://localhost:4000/jekyll/update/2018/01/08/welcome-to-jekyll.html" rel="alternate" type="text/html" title="Topic Modelling" /><published>2018-01-08T19:20:38+05:30</published><updated>2018-01-08T19:20:38+05:30</updated><id>http://localhost:4000/jekyll/update/2018/01/08/welcome-to-jekyll</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/01/08/welcome-to-jekyll.html">&lt;p&gt;Imagine that, its early morning and you are reading newspaper just dropped by delivery guy. Now while sipping a hot cup of tea, you get an idea. Why can’t I organize the articles by cool ML Techinques? The Caffaine pumped man, goes online, collects all Newspaper articles and wants them to be grouped. Thats where LDA comes as a savior. In this case, a newspaper article is document (for LDA) and what we want to do is assign a topic to each of the document. For example ‘Novak Djokovic’ retiring comes under Sports section and ‘Hillary vs Trump’ comes under politics section.&lt;/p&gt;

&lt;p&gt;Let us consider a scenario where there are n simultaneously occuring events. Each event has m catagories namely $[c_1,c_2,\cdots c_m]$ with probability $[p_1,p_2,\cdots p_m]$ respectively. Now our objective is to find the probability of category $c_1$ occuring $n_1$ times,$c_1$ occuring $n_2$ times, and so on. In other words $[c_1,c_2,\cdots c_m]$ occuring $[n_1,n_2,\cdots n_m]$. Here there two intuitive lemmas/observed constrains are as stated bellow :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum\limits_{i=0}^{i=m}n_i=n&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum\limits_{i=0}^{i=m}p_i=1&lt;/script&gt;

&lt;p&gt;This joint probability (which is our objective) is given by equation bellow :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(n_1,n_2,\cdots c_m)=\cfrac{\Gamma(n_1+n_2 + \cdots n_m+1)}{\Gamma(n_1+1)\Gamma(n_2+1)\cdots \Gamma(n_m+1)}p_1^{n_1} p_2^{n_2} \cdots p_m^{n_m}&lt;/script&gt;

&lt;p&gt;where $\Gamma(n)=(n-1)!$ or in other words  $\Gamma(n)=(n-1)(n-2)\cdots 1$.
Wait a second. Does this equation look familiar? I know its too long, but doesn’t it remind of some other equation ? Well . . . you guessed it right. It looks very similar to binomial distribution. If there are only two catagories. Here head occurs n1 times and tail occurs n-n1 times The equation becomes&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(n_1,n-n1)=\cfrac{\Gamma(n+1)}{\Gamma(n_1+1)\Gamma(n-n_1+1)}p_1^{n_1} p_2^{n-n_1}&lt;/script&gt;

&lt;p&gt;or simplifying it further it becomes&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p((n_1)_H)= ^nC_{n_1} p_1^{n_1} p_2^{n-n_1}&lt;/script&gt;

&lt;p&gt;Here $p((n_1)_H)$ is nothing but probability of getting $n_1$ heads out of n trials.&lt;/p&gt;</content><author><name></name></author><summary type="html">Imagine that, its early morning and you are reading newspaper just dropped by delivery guy. Now while sipping a hot cup of tea, you get an idea. Why can’t I organize the articles by cool ML Techinques? The Caffaine pumped man, goes online, collects all Newspaper articles and wants them to be grouped. Thats where LDA comes as a savior. In this case, a newspaper article is document (for LDA) and what we want to do is assign a topic to each of the document. For example ‘Novak Djokovic’ retiring comes under Sports section and ‘Hillary vs Trump’ comes under politics section.</summary></entry></feed>